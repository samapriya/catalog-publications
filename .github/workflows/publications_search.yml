name: Dimensions Publications Search
on: 
  workflow_dispatch:  # Allow manual trigger
  schedule:
    # Run daily at 11:15 AM CDT (4:15 PM UTC during CDT, 5:15 PM UTC during CST)
    # Note: GitHub Actions uses UTC, so adjust for your timezone
    # CDT (UTC-5): 11:15 AM CDT = 4:15 PM UTC
    # CST (UTC-6): 11:15 AM CST = 5:15 PM UTC
    - cron: '15 16 * * *'  # 4:15 PM UTC = 11:15 AM CDT

jobs:
  search_dimensions:
    runs-on: ubuntu-latest
    steps:

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python packages
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Run Dimensions search script
        uses: jannekem/run-python-script-action@v1
        env:
          # Add your Dimensions.ai authentication as GitHub secrets
          # Go to Settings > Secrets and variables > Actions > New repository secret
          DIMENSIONS_CSRF_TOKEN: ${{ secrets.DIMENSIONS_CSRF_TOKEN }}
          DIMENSIONS_AUTH_TICKET: ${{ secrets.DIMENSIONS_AUTH_TICKET }}
          DIMENSIONS_SESSION: ${{ secrets.DIMENSIONS_SESSION }}
        with:
          script: |
            import json
            import logging
            import os
            import time
            from datetime import datetime
            from typing import Any, Dict, List

            import requests

            # Set up logging
            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
            logger = logging.getLogger(__name__)

            class DimensionsSearcher:
                def __init__(self, headers: Dict[str, str], cookies: Dict[str, str], base_url: str):
                    """Initialize the Dimensions searcher with authentication details."""
                    self.headers = headers
                    self.cookies = cookies
                    self.base_url = base_url
                    self.all_results = []
                    self.search_stats = {}

                def make_search_request(self, search_text: str, search_description: str = "") -> Dict[str, Any]:
                    """Make a single search request to Dimensions.ai API."""
                    params = {
                        "search_mode": "content",
                        "search_text": search_text,
                        "search_type": "kws",
                        "search_field": "full_search",
                        "order": "date",
                    }

                    try:
                        logger.info(f"Searching for: {search_text} ({search_description})")
                        response = requests.get(self.base_url, params=params, headers=self.headers, cookies=self.cookies)
                        response.raise_for_status()

                        data = response.json()
                        count = data.get('count', 0)
                        docs = data.get('docs', [])

                        logger.info(f"Found {count} results for '{search_text}'")
                        self.search_stats[search_text] = {
                            'description': search_description,
                            'count': count,
                            'retrieved': len(docs)
                        }

                        return data

                    except requests.exceptions.RequestException as e:
                        logger.error(f"Error searching for '{search_text}': {e}")
                        self.search_stats[search_text] = {
                            'description': search_description,
                            'count': 0,
                            'retrieved': 0,
                            'error': str(e)
                        }
                        return {'count': 0, 'docs': []}

                def search_multiple_terms(self, search_terms: List[Dict[str, str]], sleep_duration: float = 2.0):
                    """Search for multiple terms with sleep between requests."""
                    logger.info(f"Starting search for {len(search_terms)} terms with {sleep_duration}s delay between requests")

                    for i, term_info in enumerate(search_terms):
                        search_text = term_info['term']
                        description = term_info.get('description', term_info.get('category', ''))

                        # Make the search request
                        result = self.make_search_request(search_text, description)

                        # Add results to our collection
                        if result.get('docs'):
                            self.all_results.extend(result['docs'])

                        # Sleep between requests (except for the last one)
                        if i < len(search_terms) - 1:
                            logger.info(f"Sleeping for {sleep_duration} seconds...")
                            time.sleep(sleep_duration)

                def filter_publication_fields(self, publication: Dict[str, Any]) -> Dict[str, Any]:
                    """Filter publication to keep only the desired fields."""
                    filtered_pub = {}
                    
                    # Handle title
                    filtered_pub['title'] = publication.get('title')
                    
                    # Handle authors - check author_list first, then editor_list if author_list is null
                    author_list = publication.get('author_list')
                    if not author_list:  # if None or empty string
                        author_list = publication.get('editor_list')
                    filtered_pub['author_list'] = author_list
                    
                    # Handle journal and source titles
                    filtered_pub['journal_title'] = publication.get('journal_title')
                    filtered_pub['source_title'] = publication.get('source_title')
                    
                    # Handle DOI
                    filtered_pub['doi'] = publication.get('doi')
                    
                    # Handle short abstract
                    filtered_pub['short_abstract'] = publication.get('short_abstract')
                    
                    # Handle publication date - choose the oldest among print_pub_date, pub_date, online_pub_date
                    date_fields = ['print_pub_date', 'pub_date', 'online_pub_date']
                    available_dates = []
                    
                    for date_field in date_fields:
                        date_value = publication.get(date_field)
                        if date_value:  # if not None and not empty
                            available_dates.append(date_value)
                    
                    # Choose the oldest (earliest) date if any are available
                    if available_dates:
                        # Sort dates and take the first (oldest)
                        try:
                            # Handle different date formats by converting to string and sorting
                            oldest_date = min(available_dates, key=lambda x: str(x))
                            filtered_pub['publication_date'] = oldest_date
                        except:
                            # If sorting fails, just take the first available date
                            filtered_pub['publication_date'] = available_dates[0]
                    else:
                        filtered_pub['publication_date'] = None
                            
                    return filtered_pub

                def remove_duplicates(self) -> List[Dict[str, Any]]:
                    """Remove duplicate publications based on their ID and filter fields."""
                    seen_ids = set()
                    unique_results = []

                    logger.info(f"Removing duplicates from {len(self.all_results)} total results")

                    for doc in self.all_results:
                        doc_id = doc.get('id')
                        if doc_id and doc_id not in seen_ids:
                            seen_ids.add(doc_id)
                            # Filter the publication to keep only desired fields
                            filtered_pub = self.filter_publication_fields(doc)
                            unique_results.append(filtered_pub)

                    logger.info(f"After deduplication and filtering: {len(unique_results)} unique publications")
                    return unique_results

                def save_results(self, filename: str = None) -> str:
                    """Save the filtered unique publications to a JSON file."""
                    if filename is None:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"dimensions_filtered_results.json"

                    unique_results = self.remove_duplicates()

                    # Save only the unique publications array (no metadata)
                    final_output = unique_results

                    try:
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(final_output, f, indent=2, ensure_ascii=False)

                        logger.info(f"Filtered results saved to {filename}")
                        logger.info(f"Total publications saved: {len(unique_results)}")
                        
                        # Also create a latest version for easy access
                        latest_filename = "dimensions_filtered_results_latest.json"
                        with open(latest_filename, 'w', encoding='utf-8') as f:
                            json.dump(final_output, f, indent=2, ensure_ascii=False)
                        logger.info(f"Latest version saved to {latest_filename}")
                        
                        return filename

                    except Exception as e:
                        logger.error(f"Error saving results: {e}")
                        raise

                def save_detailed_results(self, filename: str = None) -> str:
                    """Save results with metadata (original format) to a separate file."""
                    if filename is None:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"dimensions_detailed_results.json"

                    unique_results_full = []
                    seen_ids = set()
                    
                    for doc in self.all_results:
                        doc_id = doc.get('id')
                        if doc_id and doc_id not in seen_ids:
                            seen_ids.add(doc_id)
                            unique_results_full.append(doc)

                    final_output = {
                        'search_metadata': {
                            'total_searches': len(self.search_stats),
                            'total_results_before_dedup': len(self.all_results),
                            'total_unique_results': len(unique_results_full),
                            'search_timestamp': datetime.now().isoformat(),
                            'search_statistics': self.search_stats
                        },
                        'unique_publications': unique_results_full
                    }

                    try:
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(final_output, f, indent=2, ensure_ascii=False)

                        logger.info(f"Detailed results saved to {filename}")
                        
                        # Also create a latest version
                        latest_filename = "dimensions_detailed_results_latest.json"
                        with open(latest_filename, 'w', encoding='utf-8') as f:
                            json.dump(final_output, f, indent=2, ensure_ascii=False)
                        
                        return filename

                    except Exception as e:
                        logger.error(f"Error saving detailed results: {e}")
                        raise

            def main():
                # Get authentication details from environment variables (for GitHub Actions)
                # or use default values (for local testing)
                csrf_token = os.getenv('DIMENSIONS_CSRF_TOKEN')
                auth_ticket = os.getenv('DIMENSIONS_AUTH_TICKET')
                session_cookie = os.getenv('DIMENSIONS_SESSION')
                
                headers = {
                    "accept": "application/json",
                    "accept-language": "en-US,en;q=0.7",
                    "cache-control": "no-cache, no-store, must-revalidate",
                    "content-type": "application/json",
                    "expires": "0",
                    "operations": "sticky=nI4fV5SGzbBXzHTwShz8rDXqRjfK1VznwaNnBgpi",
                    "pragma": "no-cache",
                    "priority": "u=1, i",
                    "referer": "https://app.dimensions.ai/discover/publication_plus?search_mode=content&search_text=%22gee%20community%20catalog%22&search_type=kws&search_field=full_search&order=date",
                    "sec-ch-ua": '"Brave";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
                    "sec-ch-ua-mobile": "?0",
                    "sec-ch-ua-platform": '"Windows"',
                    "sec-fetch-dest": "empty",
                    "sec-fetch-mode": "cors",
                    "sec-fetch-site": "same-origin",
                    "sec-gpc": "1",
                    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
                    "x-csrf-token": csrf_token,
                    "x-requested-with": "XMLHttpRequest",
                }

                cookies = {
                    "uber_auth_tkt": auth_ticket,
                    "session": session_cookie,
                }

                # Define all search terms
                search_terms = [
                    # High-precision terms (URLs and exact names)
                    {'term': '"gee-community-catalog.org"', 'category': 'Official URL', 'description': 'Main website URL'},
                    {'term': '"Awesome GEE Community Catalog"', 'category': 'Official Name', 'description': 'Full official name'},
                    {'term': '"awesome-gee-community-datasets"', 'category': 'Repository Name', 'description': 'GitHub repository name'},
                    {'term': '"Samapriya/Awesome-Gee-Community-Datasets"', 'category': 'GitHub Reference', 'description': 'Full GitHub path'},
                    {'term': '"github.com/samapriya/awesome-gee-community-datasets"', 'category': 'GitHub URL', 'description': 'Full GitHub URL'},
                    {'term': '"https://github.com/samapriya/awesome-gee-community-datasets"', 'category': 'GitHub URL with protocol', 'description': 'Complete GitHub URL'},
                    {'term': '"GEE Community Datasets"', 'category': 'Generic Name', 'description': 'Generic name for datasets'},
                    {'term': '"Earth Engine Community Catalog"', 'category': 'Specific Name', 'description': 'Earth Engine specific catalog reference'},
                    {'term': '"Earth Engine Community Catalogue"', 'category': 'Specific Name', 'description': 'Alternative spelling with Earth Engine'},
                    {'term': '"GEE Community Assets"', 'category': 'Generic Name', 'description': 'Community assets reference'},
                    {'term': '"projects/sat-io"', 'category': 'Project Path', 'description': 'Satellite IO project path'},                ]

                # Initialize searcher
                base_url = "https://app.dimensions.ai/discover/publication/results.json"
                searcher = DimensionsSearcher(headers, cookies, base_url)

                # Perform searches with 3-second delay between requests
                searcher.search_multiple_terms(search_terms, sleep_duration=3.0)

                # Save filtered results (only essential fields)
                filtered_output_file = searcher.save_results()
                
                # Optionally save detailed results with metadata
                detailed_output_file = searcher.save_detailed_results()

                # Print summary
                print("\n" + "="*50)
                print("SEARCH SUMMARY")
                print("="*50)
                print(f"Total search terms: {len(search_terms)}")
                print(f"Total results before deduplication: {len(searcher.all_results)}")
                unique_results = searcher.remove_duplicates()
                print(f"Unique publications after deduplication: {len(unique_results)}")
                print(f"Filtered results saved to: {filtered_output_file}")
                print(f"Detailed results saved to: {detailed_output_file}")

                # Print search statistics
                print("\nDETAILED SEARCH STATISTICS:")
                print("-" * 50)
                for search_text, stats in searcher.search_stats.items():
                    status = "âœ“" if stats['count'] > 0 else "âœ—" if 'error' not in stats else "ERROR"
                    print(f"{status} {search_text[:50]:<50} | {stats['count']:>3} results | {stats.get('description', '')}")

                # Show sample of filtered data
                if unique_results:
                    print(f"\nSAMPLE FILTERED PUBLICATION:")
                    print("-" * 50)
                    sample_pub = unique_results[0]
                    for field, value in sample_pub.items():
                        print(f"{field}: {value}")

                return filtered_output_file, detailed_output_file

            if __name__ == "__main__":
                try:
                    # Check if we're running in GitHub Actions
                    is_github_actions = os.getenv('GITHUB_ACTIONS') == 'true'
                    
                    if is_github_actions:
                        logger.info("Running in GitHub Actions environment")
                        
                        # Check if required environment variables are set
                        required_vars = ['DIMENSIONS_CSRF_TOKEN', 'DIMENSIONS_AUTH_TICKET', 'DIMENSIONS_SESSION']
                        missing_vars = [var for var in required_vars if not os.getenv(var)]
                        
                        if missing_vars:
                            logger.warning(f"Missing environment variables: {missing_vars}")
                            logger.warning("The script will use default values, which may not work")
                    
                    filtered_file, detailed_file = main()
                    print(f"\nðŸŽ‰ Search completed successfully!")
                    print(f"ðŸ“„ Filtered results: {filtered_file}")
                    print(f"ðŸ“‹ Detailed results: {detailed_file}")
                    
                except Exception as e:
                    logger.error(f"Script failed: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
                    raise

            # Execute the main function
            main()

      - name: Check generated files
        run: |
          ls -la *.json || echo "No JSON files found"
          echo "=== Filtered Results ==="
          if [ -f dimensions_filtered_results_*.json ]; then
            echo "Found filtered results files:"
            ls -la dimensions_filtered_results_*.json
            echo "Number of publications:"
            python -c "import json; print(len(json.load(open([f for f in __import__('glob').glob('dimensions_filtered_results_*.json') if 'latest' not in f][0]))))" 2>/dev/null || echo "Could not count publications"
          else
            echo "No filtered results files found"
          fi

      - name: Commit and push results
        continue-on-error: true
        run: |
          today=$(date +"%Y-%m-%d")
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Add only the JSON result files
          git add dimensions_filtered_results_*.json dimensions_detailed_results_*.json 2>/dev/null || echo "No result files to add"
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update Dimensions GEE search results ${today} UTC"
            
            # Pull latest changes and push
            git pull origin main --rebase || git pull origin master --rebase
            git push origin HEAD || echo "Failed to push changes"
          fi
