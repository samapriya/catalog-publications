name: Dimensions Publications Search
on: 
  workflow_dispatch:  # Allow manual trigger
  schedule:
    # Run daily at 11:15 AM CDT (4:15 PM UTC during CDT, 5:15 PM UTC during CST)
    # Note: GitHub Actions uses UTC, so adjust for your timezone
    # CDT (UTC-5): 11:15 AM CDT = 4:15 PM UTC
    # CST (UTC-6): 11:15 AM CST = 5:15 PM UTC
    - cron: '15 16 * * *'  # 4:15 PM UTC = 11:15 AM CDT

jobs:
  search_dimensions:
    runs-on: ubuntu-latest
    steps:

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python packages
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Run Dimensions search script
        uses: jannekem/run-python-script-action@v1
        env:
          # Add your Dimensions.ai authentication as GitHub secrets
          # Go to Settings > Secrets and variables > Actions > New repository secret
          DIMENSIONS_CSRF_TOKEN: ${{ secrets.DIMENSIONS_CSRF_TOKEN }}
          DIMENSIONS_AUTH_TICKET: ${{ secrets.DIMENSIONS_AUTH_TICKET }}
          DIMENSIONS_SESSION: ${{ secrets.DIMENSIONS_SESSION }}
        with:
          script: |
            import json
            import logging
            import os
            import re
            import time
            from datetime import datetime
            from typing import Any, Dict, List

            import requests

            # Set up logging
            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
            logger = logging.getLogger(__name__)

            class DimensionsSearcher:
                def __init__(self, headers: Dict[str, str], cookies: Dict[str, str], base_url: str):
                    """Initialize the Dimensions searcher with authentication details."""
                    self.headers = headers
                    self.cookies = cookies
                    self.base_url = base_url
                    self.all_results = []
                    self.search_stats = {}

                @staticmethod
                def clean_html_markup(text: str) -> str:
                    """Remove HTML markup from text, specifically search keyword highlighting."""
                    if not text or not isinstance(text, str):
                        return text
                    
                    # Remove <mark class="search-keyword"> tags and </mark> tags
                    cleaned = re.sub(r'<mark class="search-keyword">', '', text)
                    cleaned = re.sub(r'</mark>', '', cleaned)
                    
                    # Remove any other HTML tags as a fallback
                    cleaned = re.sub(r'<[^>]+>', '', cleaned)
                    
                    return cleaned.strip()

                def make_search_request(self, search_text: str, search_description: str = "", search_field: str = "full_search") -> Dict[str, Any]:
                    """Make a single search request to Dimensions.ai API."""
                    params = {
                        "search_mode": "content",
                        "search_text": search_text,
                        "search_type": "kws",
                        "search_field": search_field,
                        "order": "date",
                    }

                    try:
                        logger.info(f"Searching for: {search_text} ({search_description}) [field: {search_field}]")
                        response = requests.get(self.base_url, params=params, headers=self.headers, cookies=self.cookies)
                        response.raise_for_status()

                        data = response.json()
                        count = data.get('count', 0)
                        docs = data.get('docs', [])

                        # Filter docs by publication type and select best matches
                        filtered_docs = self.filter_and_select_best_docs(docs)

                        logger.info(f"Found {count} total results, {len(filtered_docs)} after filtering for '{search_text}'")
                        self.search_stats[search_text] = {
                            'description': search_description,
                            'search_field': search_field,
                            'count': count,
                            'retrieved': len(docs),
                            'filtered': len(filtered_docs)
                        }

                        # Return modified data with filtered docs
                        data['docs'] = filtered_docs
                        return data

                    except requests.exceptions.RequestException as e:
                        logger.error(f"Error searching for '{search_text}': {e}")
                        self.search_stats[search_text] = {
                            'description': search_description,
                            'search_field': search_field,
                            'count': 0,
                            'retrieved': 0,
                            'filtered': 0,
                            'error': str(e)
                        }
                        return {'count': 0, 'docs': []}

                def filter_and_select_best_docs(self, docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
                    """Filter docs by publication type and select best matches based on score and type priority."""
                    if not docs:
                        return []

                    # Filter by allowed publication types (no abstracts)
                    allowed_types = {"article", "preprint", "book", "chapter"}
                    filtered_docs = []
                    
                    for doc in docs:
                        pub_class_id = doc.get('pub_class_id')
                        if pub_class_id in allowed_types:
                            filtered_docs.append(doc)

                    if not filtered_docs:
                        return []

                    # Group by cleaned title to handle duplicates
                    title_groups = {}
                    for doc in filtered_docs:
                        cleaned_title = self.clean_html_markup(doc.get('title', '')).lower().strip()
                        if cleaned_title:
                            if cleaned_title not in title_groups:
                                title_groups[cleaned_title] = []
                            title_groups[cleaned_title].append(doc)

                    # Select best document for each title
                    selected_docs = []
                    for title, doc_group in title_groups.items():
                        if len(doc_group) == 1:
                            selected_docs.append(doc_group[0])
                        else:
                            # Multiple docs with same title - prioritize article over preprint, then by score
                            best_doc = self.select_best_document(doc_group)
                            selected_docs.append(best_doc)

                    # Sort by score descending and return only the highest scoring document
                    selected_docs.sort(key=lambda x: x.get('score', 0), reverse=True)
                    
                    # Return only the single best document per search
                    return selected_docs[:1] if selected_docs else []

                def select_best_document(self, doc_group: List[Dict[str, Any]]) -> Dict[str, Any]:
                    """Select the best document from a group with the same title."""
                    # Separate by publication type
                    articles = [doc for doc in doc_group if doc.get('pub_class_id') == 'article']
                    preprints = [doc for doc in doc_group if doc.get('pub_class_id') == 'preprint']
                    books = [doc for doc in doc_group if doc.get('pub_class_id') == 'book']
                    chapters = [doc for doc in doc_group if doc.get('pub_class_id') == 'chapter']

                    # Priority: article > book > chapter > preprint
                    for doc_list in [articles, books, chapters, preprints]:
                        if doc_list:
                            # Within same type, select highest score
                            best_in_type = max(doc_list, key=lambda x: x.get('score', 0))
                            return best_in_type

                    # Fallback: return highest score overall
                    return max(doc_group, key=lambda x: x.get('score', 0))

                def search_multiple_terms(self, search_terms: List[Dict[str, str]], sleep_duration: float = 2.0):
                    """Search for multiple terms with sleep between requests."""
                    logger.info(f"Starting keyword search for {len(search_terms)} terms with {sleep_duration}s delay between requests")

                    for i, term_info in enumerate(search_terms):
                        search_text = term_info['term']
                        description = term_info.get('description', term_info.get('category', ''))

                        # Keyword searches always use full_search
                        result = self.make_search_request(search_text, description, "full_search")

                        # Add results to our collection
                        if result.get('docs'):
                            self.all_results.extend(result['docs'])

                        # Sleep between requests (except for the last one)
                        if i < len(search_terms) - 1:
                            logger.info(f"Sleeping for {sleep_duration} seconds...")
                            time.sleep(sleep_duration)

                def fetch_and_search_scholar_diff(self, url: str, sleep_duration: float = 2.0):
                    """Fetch scholar diff titles and search for them using text_search field."""
                    try:
                        logger.info(f"Fetching scholar diff titles from: {url}")
                        response = requests.get(url, timeout=30)
                        response.raise_for_status()
                        
                        scholar_titles = response.json()
                        logger.info(f"Found {len(scholar_titles)} scholar titles to search")
                        
                        # Search for each title using text_search field (fixed for title searches)
                        for i, title in enumerate(scholar_titles):
                            # Title searches always use text_search
                            result = self.make_search_request(title, "Scholar Diff Title", "text_search")
                            
                            # Add results to our collection
                            if result.get('docs'):
                                self.all_results.extend(result['docs'])
                            
                            # Sleep between requests (except for the last one)
                            if i < len(scholar_titles) - 1:
                                logger.info(f"Sleeping for {sleep_duration} seconds...")
                                time.sleep(sleep_duration)
                                
                    except requests.exceptions.RequestException as e:
                        logger.error(f"Error fetching scholar diff titles: {e}")
                    except json.JSONDecodeError as e:
                        logger.error(f"Error parsing scholar diff JSON: {e}")
                    except Exception as e:
                        logger.error(f"Unexpected error processing scholar diff: {e}")

                def filter_publication_fields(self, publication: Dict[str, Any]) -> Dict[str, Any]:
                    """Filter publication to keep only the desired fields and clean HTML markup."""
                    filtered_pub = {}
                    
                    # Handle title - clean HTML markup
                    title = publication.get('title')
                    filtered_pub['title'] = self.clean_html_markup(title)
                    
                    # Handle authors - check author_list first, then editor_list if author_list is null
                    author_list = publication.get('author_list')
                    if not author_list:  # if None or empty string
                        author_list = publication.get('editor_list')
                    filtered_pub['author_list'] = self.clean_html_markup(author_list)
                    
                    # Handle journal and source titles - clean HTML markup
                    journal_title = publication.get('journal_title')
                    filtered_pub['journal_title'] = self.clean_html_markup(journal_title)
                    
                    source_title = publication.get('source_title')
                    filtered_pub['source_title'] = self.clean_html_markup(source_title)
                    
                    # Handle DOI - clean HTML markup
                    doi = publication.get('doi')
                    filtered_pub['doi'] = self.clean_html_markup(doi)
                    
                    # Handle short abstract - clean HTML markup
                    short_abstract = publication.get('short_abstract')
                    filtered_pub['short_abstract'] = self.clean_html_markup(short_abstract)
                    
                    # Handle publication date - choose the oldest among print_pub_date, pub_date, online_pub_date
                    date_fields = ['print_pub_date', 'pub_date', 'online_pub_date']
                    available_dates = []
                    
                    for date_field in date_fields:
                        date_value = publication.get(date_field)
                        if date_value:  # if not None and not empty
                            available_dates.append(date_value)
                    
                    # Choose the oldest (earliest) date if any are available
                    if available_dates:
                        # Sort dates and take the first (oldest)
                        try:
                            # Handle different date formats by converting to string and sorting
                            oldest_date = min(available_dates, key=lambda x: str(x))
                            filtered_pub['publication_date'] = oldest_date
                        except:
                            # If sorting fails, just take the first available date
                            filtered_pub['publication_date'] = available_dates[0]
                    else:
                        filtered_pub['publication_date'] = None
                            
                    return filtered_pub

                def remove_duplicates(self) -> List[Dict[str, Any]]:
                    """Remove duplicate publications based on their ID and filter fields."""
                    seen_ids = set()
                    unique_results = []

                    logger.info(f"Removing duplicates from {len(self.all_results)} total results")

                    for doc in self.all_results:
                        doc_id = doc.get('id')
                        if doc_id and doc_id not in seen_ids:
                            seen_ids.add(doc_id)
                            # Filter the publication to keep only desired fields
                            filtered_pub = self.filter_publication_fields(doc)
                            unique_results.append(filtered_pub)

                    logger.info(f"After deduplication and filtering: {len(unique_results)} unique publications")
                    return unique_results

                def save_results(self, filename: str = None) -> str:
                    """Save the filtered unique publications to a JSON file."""
                    if filename is None:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"dimensions_filtered_results.json"

                    unique_results = self.remove_duplicates()

                    # Save only the unique publications array (no metadata)
                    final_output = unique_results

                    try:
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(final_output, f, indent=2, ensure_ascii=False)

                        logger.info(f"Filtered results saved to {filename}")
                        logger.info(f"Total publications saved: {len(unique_results)}")
                        
                        # Also create a latest version for easy access
                        latest_filename = "dimensions_filtered_results_latest.json"
                        with open(latest_filename, 'w', encoding='utf-8') as f:
                            json.dump(final_output, f, indent=2, ensure_ascii=False)
                        logger.info(f"Latest version saved to {latest_filename}")
                        
                        return filename

                    except Exception as e:
                        logger.error(f"Error saving results: {e}")
                        raise

                def save_failed_searches(self, filename: str = None) -> str:
                    """Save failed searches to a separate JSON file."""
                    if filename is None:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"dimensions_failed_searches.json"

                    # Filter out only the failed searches
                    failed_searches = {}
                    for search_text, stats in self.search_stats.items():
                        if 'error' in stats or stats.get('count', 0) == 0:
                            failed_searches[search_text] = stats

                    if not failed_searches:
                        logger.info("No failed searches to save")
                        return None

                    failed_output = {
                        'failed_search_metadata': {
                            'total_failed_searches': len(failed_searches),
                            'export_timestamp': datetime.now().isoformat(),
                        },
                        'failed_searches': failed_searches
                    }

                    try:
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(failed_output, f, indent=2, ensure_ascii=False)

                        logger.info(f"Failed searches saved to {filename}")
                        logger.info(f"Total failed searches: {len(failed_searches)}")
                        
                        # Also create a latest version
                        latest_filename = "dimensions_failed_searches_latest.json"
                        with open(latest_filename, 'w', encoding='utf-8') as f:
                            json.dump(failed_output, f, indent=2, ensure_ascii=False)
                        logger.info(f"Latest failed searches saved to {latest_filename}")
                        
                        return filename

                    except Exception as e:
                        logger.error(f"Error saving failed searches: {e}")
                        raise
                    """Save results with metadata (original format) to a separate file."""
                    if filename is None:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"dimensions_detailed_results.json"

                    unique_results_full = []
                    seen_ids = set()
                    
                    for doc in self.all_results:
                        doc_id = doc.get('id')
                        if doc_id and doc_id not in seen_ids:
                            seen_ids.add(doc_id)
                            unique_results_full.append(doc)

                    final_output = {
                        'search_metadata': {
                            'total_searches': len(self.search_stats),
                            'total_results_before_dedup': len(self.all_results),
                            'total_unique_results': len(unique_results_full),
                            'search_timestamp': datetime.now().isoformat(),
                            'search_statistics': self.search_stats
                        },
                        'unique_publications': unique_results_full
                    }

                    try:
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(final_output, f, indent=2, ensure_ascii=False)

                        logger.info(f"Detailed results saved to {filename}")
                        
                        # Also create a latest version
                        latest_filename = "dimensions_detailed_results_latest.json"
                        with open(latest_filename, 'w', encoding='utf-8') as f:
                            json.dump(final_output, f, indent=2, ensure_ascii=False)
                        
                        return filename

                    except Exception as e:
                        logger.error(f"Error saving detailed results: {e}")
                        raise

            def main():
                # Get authentication details from environment variables (for GitHub Actions)
                # or use default values (for local testing)
                csrf_token = os.getenv('DIMENSIONS_CSRF_TOKEN')
                auth_ticket = os.getenv('DIMENSIONS_AUTH_TICKET')
                session_cookie = os.getenv('DIMENSIONS_SESSION')
                
                headers = {
                    "accept": "application/json",
                    "accept-language": "en-US,en;q=0.7",
                    "cache-control": "no-cache, no-store, must-revalidate",
                    "content-type": "application/json",
                    "expires": "0",
                    "operations": "sticky=nI4fV5SGzbBXzHTwShz8rDXqRjfK1VznwaNnBgpi",
                    "pragma": "no-cache",
                    "priority": "u=1, i",
                    "referer": "https://app.dimensions.ai/discover/publication_plus?search_mode=content&search_text=%22gee%20community%20catalog%22&search_type=kws&search_field=full_search&order=date",
                    "sec-ch-ua": '"Brave";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
                    "sec-ch-ua-mobile": "?0",
                    "sec-ch-ua-platform": '"Windows"',
                    "sec-fetch-dest": "empty",
                    "sec-fetch-mode": "cors",
                    "sec-fetch-site": "same-origin",
                    "sec-gpc": "1",
                    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
                    "x-csrf-token": csrf_token,
                    "x-requested-with": "XMLHttpRequest",
                }

                cookies = {
                    "uber_auth_tkt": auth_ticket,
                    "session": session_cookie,
                }

                # Define all search terms
                search_terms = [
                    # High-precision terms (URLs and exact names)
                    {'term': '"gee-community-catalog.org"', 'category': 'Official URL', 'description': 'Main website URL'},
                    {'term': '"Awesome GEE Community Catalog"', 'category': 'Official Name', 'description': 'Full official name'},
                    {'term': '"awesome-gee-community-datasets"', 'category': 'Repository Name', 'description': 'GitHub repository name'},
                    {'term': '"Samapriya/Awesome-Gee-Community-Datasets"', 'category': 'GitHub Reference', 'description': 'Full GitHub path'},
                    {'term': '"github.com/samapriya/awesome-gee-community-datasets"', 'category': 'GitHub URL', 'description': 'Full GitHub URL'},
                    {'term': '"https://github.com/samapriya/awesome-gee-community-datasets"', 'category': 'GitHub URL with protocol', 'description': 'Complete GitHub URL'},
                    {'term': '"GEE Community Datasets"', 'category': 'Generic Name', 'description': 'Generic name for datasets'},
                    {'term': '"Earth Engine Community Catalog"', 'category': 'Specific Name', 'description': 'Earth Engine specific catalog reference'},
                    {'term': '"Earth Engine Community Catalogue"', 'category': 'Specific Name', 'description': 'Alternative spelling with Earth Engine'},
                    {'term': '"GEE Community Assets"', 'category': 'Generic Name', 'description': 'Community assets reference'},
                    {'term': '"projects/sat-io"', 'category': 'Project Path', 'description': 'Satellite IO project path'},
                ]

                # Initialize searcher
                base_url = "https://app.dimensions.ai/discover/publication/results.json"
                searcher = DimensionsSearcher(headers, cookies, base_url)

                # Perform keyword searches with 3-second delay between requests
                searcher.search_multiple_terms(search_terms, sleep_duration=3.0)

                # Sleep before starting scholar diff searches
                logger.info("Sleeping for 3 seconds before starting scholar diff searches...")
                time.sleep(3.0)

                # Fetch and search scholar diff titles
                scholar_diff_url = "https://raw.githubusercontent.com/samapriya/catalog-publications/refs/heads/main/scholar_diff.json"
                searcher.fetch_and_search_scholar_diff(scholar_diff_url, sleep_duration=3.0)

                # Save filtered results (only essential fields)
                filtered_output_file = searcher.save_results()
                
                # Save detailed results with metadata
                detailed_output_file = searcher.save_detailed_results()

                # Save failed searches
                failed_searches_file = searcher.save_failed_searches()

                # Print summary
                print("\n" + "="*50)
                print("SEARCH SUMMARY")
                print("="*50)
                print(f"Total search terms: {len(search_terms)}")
                print(f"Total results before deduplication: {len(searcher.all_results)}")
                unique_results = searcher.remove_duplicates()
                print(f"Unique publications after deduplication: {len(unique_results)}")
                print(f"Filtered results saved to: {filtered_output_file}")
                print(f"Detailed results saved to: {detailed_output_file}")
                if failed_searches_file:
                    print(f"Failed searches saved to: {failed_searches_file}")
                else:
                    print("No failed searches to save")

                # Print search statistics
                print("\nDETAILED SEARCH STATISTICS:")
                print("-" * 80)
                for search_text, stats in searcher.search_stats.items():
                    status = "✓" if stats.get('filtered', 0) > 0 else "✗" if 'error' not in stats else "ERROR"
                    field_info = f"[{stats.get('search_field', 'full_search')}]"
                    total = stats.get('count', 0)
                    retrieved = stats.get('retrieved', 0)
                    filtered = stats.get('filtered', 0)
                    print(f"{status} {search_text[:35]:<35} {field_info:<15} | {total:>3} total → {retrieved:>3} retrieved → {filtered:>3} filtered | {stats.get('description', '')}")

                # Show sample of filtered data
                if unique_results:
                    print(f"\nSAMPLE FILTERED PUBLICATION:")
                    print("-" * 50)
                    sample_pub = unique_results[0]
                    for field, value in sample_pub.items():
                        print(f"{field}: {value}")

                return filtered_output_file, detailed_output_file, failed_searches_file

            if __name__ == "__main__":
                try:
                    # Check if we're running in GitHub Actions
                    is_github_actions = os.getenv('GITHUB_ACTIONS') == 'true'
                    
                    if is_github_actions:
                        logger.info("Running in GitHub Actions environment")
                        
                        # Check if required environment variables are set
                        required_vars = ['DIMENSIONS_CSRF_TOKEN', 'DIMENSIONS_AUTH_TICKET', 'DIMENSIONS_SESSION']
                        missing_vars = [var for var in required_vars if not os.getenv(var)]
                        
                        if missing_vars:
                            logger.warning(f"Missing environment variables: {missing_vars}")
                            logger.warning("The script will use default values, which may not work")
                    
                    filtered_file, detailed_file, failed_file = main()
                    print(f"\n🎉 Search completed successfully!")
                    print(f"📄 Filtered results: {filtered_file}")
                    print(f"📋 Detailed results: {detailed_file}")
                    if failed_file:
                        print(f"❌ Failed searches: {failed_file}")
                    else:
                        print("✅ No failed searches!")
                    
                except Exception as e:
                    logger.error(f"Script failed: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
                    raise

      - name: Check generated files
        run: |
          ls -la *.json || echo "No JSON files found"
          echo "=== Filtered Results ==="
          if [ -f dimensions_filtered_results_*.json ]; then
            echo "Found filtered results files:"
            ls -la dimensions_filtered_results_*.json
            echo "Number of publications:"
            python -c "import json; print(len(json.load(open([f for f in __import__('glob').glob('dimensions_filtered_results_*.json') if 'latest' not in f][0]))))" 2>/dev/null || echo "Could not count publications"
          else
            echo "No filtered results files found"
          fi

      - name: Commit and push results
        continue-on-error: true
        run: |
          today=$(date +"%Y-%m-%d")
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Add only the JSON result files
          git add dimensions_filtered_results_*.json dimensions_detailed_results_*.json dimensions_failed_searches_*.json 2>/dev/null || echo "No result files to add"
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update Dimensions GEE search results ${today} UTC"
            
            # Pull latest changes and push
            git pull origin main --rebase || git pull origin master --rebase
            git push origin HEAD || echo "Failed to push changes"
          fi
